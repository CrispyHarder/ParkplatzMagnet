{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold_daylight_for_classification = 800 # for trivial labeling \n",
    "data_split_seed = random.randint(0,1000000) # get a seed \n",
    "split_train_test = 0.1 #test_train split \n",
    "lightchange_threshhold = 300 # when a light jump is relevant \n",
    "NUMBER_OF_DAYS = 150 #number of possible days to get daylight points \n",
    "LIST_OF_BAD_DAYS = [1,4,6,13,64] #list of days, that have bad data, cause someone played with magnets \n",
    "PERCENTAGE_OF_UNSEEN_DAYS = 0.3 #how many days are not even in train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Usefull functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(gold,pred, reverse=False):\n",
    "    # reverse=True if the labels are switched around\n",
    "    if len(gold)==len(pred):\n",
    "        length = len(gold)\n",
    "        count = 0\n",
    "        for i in range(length):\n",
    "            if gold[i]==pred[i]:\n",
    "                count+=1\n",
    "            else:\n",
    "                count = count\n",
    "        score = count/length\n",
    "        if reverse:\n",
    "            return (1-score) \n",
    "        else: \n",
    "            return score\n",
    "    else : \n",
    "        error(\"Die Listen müssen die gleiche Länge haben\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data_light_time_fwd(vec):\n",
    "    ### vec should be day_data_withlight_withtime, returns day_y with forwardcheck###\n",
    "    \n",
    "    length = len(vec)\n",
    "    new_day = True\n",
    "    \n",
    "    #initiation of the vector that contains the labels \n",
    "    fwd_y = np.zeros(length)\n",
    "    \n",
    "    #we start iteration at index 1, so we label fwd_y[0] manually -1\n",
    "    fwd_y[0] = -1\n",
    "\n",
    "    \n",
    "    #iterate over timepoints\n",
    "    for i in range(1,length):\n",
    "        \n",
    "        #get features for singe forward check label \n",
    "        prev_time = vec[i-1,4]\n",
    "        time = vec[i,4]\n",
    "        prev_light = vec[i-1,3]\n",
    "        light = vec[i,3]\n",
    "        prev_label = fwd_y[i-1]\n",
    "    \n",
    "        #test if its still a new day\n",
    "        if abs(light-prev_light)>=lightchange_threshhold:\n",
    "            new_day = False\n",
    "        \n",
    "        #test if a new day started, overwrites above\n",
    "        if time-prev_time >= 1000:\n",
    "            new_day = True\n",
    "        \n",
    "        #if its a new day,label as -1 \n",
    "        if new_day:\n",
    "            fwd_y[i] = -1 \n",
    "        \n",
    "        #else get label 0 or 1 \n",
    "        else:    \n",
    "            if prev_light-light >= lightchange_threshhold:\n",
    "                fwd_y[i] = 1\n",
    "            elif light - prev_light >= lightchange_threshhold:\n",
    "                fwd_y[i] = 0 \n",
    "            else: \n",
    "                fwd_y[i] = prev_label\n",
    "    \n",
    "    return fwd_y\n",
    "            \n",
    "\n",
    "\n",
    "def label_data_light_time_bwd(vec,fwd_y):\n",
    "    \n",
    "    length = len(fwd_y)\n",
    "    label_set = False\n",
    "    label = 0\n",
    "    bwd_y = fwd_y\n",
    "    \n",
    "    #iterate backwards through fwd_y\n",
    "    for i in range(length-2,-1,-1):\n",
    "        \n",
    "        #get features \n",
    "        next_time = vec[i+1,4]\n",
    "        time = vec[i,4]\n",
    "        \n",
    "        #test if the day ends here\n",
    "        #if yes, do nothing label-wise, but reset label_set to False\n",
    "        if next_time-time >= 1000:\n",
    "            label_set = False\n",
    "        #else go through labels\n",
    "        else:\n",
    "            #if label -1 get opposite label of times before\n",
    "            if bwd_y[i] == -1: \n",
    "                if label_set:\n",
    "                    bwd_y[i] = label\n",
    "                else: \n",
    "                    if bwd_y[i+1] == -1:\n",
    "                        label = -1\n",
    "                    else:\n",
    "                        label = 1-bwd_y[i+1]\n",
    "                    bwd_y[i] = label\n",
    "                    label_set = True\n",
    "    return bwd_y\n",
    "\n",
    "def label_data_light_time_spec(vec,bwd_y):\n",
    "    \n",
    "    length = len(vec)\n",
    "    day_y = np.copy(bwd_y)\n",
    "    \n",
    "    #find start indices\n",
    "    #initiate vector with day_starts \n",
    "    day_starts = np.array([0])\n",
    "    #find other day_starts\n",
    "    for i in range(1,length):\n",
    "        if vec[i,4]-vec[i-1,4] >= 1000:\n",
    "            day_starts = np.append(day_starts, np.array([i]))\n",
    "    \n",
    "    #in order to iterate over last day wothout problems, add another index to day starts, so last day has an end\n",
    "    day_starts = np.append(day_starts, np.array([length]))\n",
    "    \n",
    "    # go through day_starts; if a label is -1, the whole day is unlabeled and has to be labeled \n",
    "    for i in range(len(day_starts)-1):\n",
    "        index = day_starts[i]\n",
    "        next_index = day_starts[i+1]\n",
    "        if bwd_y[index] == -1: \n",
    "            mean_of_day = np.mean(vec[index:next_index,3])\n",
    "            if mean_of_day <= 600:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            for i in range (index,next_index):\n",
    "                day_y[i] = label\n",
    "    return day_y\n",
    "    \n",
    "def label_data_light_time_full(vec):\n",
    "    fwd_y = label_data_light_time_fwd(vec)\n",
    "    bwd_y = label_data_light_time_bwd(vec,fwd_y)\n",
    "    day_y = label_data_light_time_spec(vec,bwd_y)\n",
    "    return day_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for result analysis\n",
    "def get_label(tupel):\n",
    "    pred = tupel[0]\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] >= 0.5:\n",
    "            pred[i] = 1 \n",
    "        else:\n",
    "            pred[i] = 0\n",
    "    return pred\n",
    "\n",
    "def label_to_num(tupel):\n",
    "    return np.argmax(tupel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing_of_labels(labels_of_aday,window_size=4):\n",
    "    \n",
    "    labels_of_aday = np.array(labels_of_aday)\n",
    "    length = len(labels_of_aday)\n",
    "    \n",
    "    #clean up single spikes\n",
    "    #cleaned_labels = np.copy(labels_of_aday)\n",
    "    #for i in range(1,length-1):\n",
    "        #if (labels_of_aday[i-1] == labels_of_aday[i+1]) and (labels_of_aday[i] != labels_of_aday[i-1]):\n",
    "            #cleaned_labels[i] = labels_of_aday[i-1]\n",
    "        #else : \n",
    "            #cleaned_labels[i] = labels_of_aday[i]\n",
    "        #print(i,labels_of_aday,cleaned_labels,labels_of_aday[i-1] == labels_of_aday[i+1])\n",
    "    #print(cleaned_labels)\n",
    "    #print(get_score(labels_of_aday,cleaned_labels))\n",
    "    \n",
    "    #for using parking\n",
    "    postp_labels = np.copy(labels_of_aday)\n",
    "    if (labels_of_aday[:window_size] == 0).all(): \n",
    "        label = 0\n",
    "    elif (labels_of_aday[:window_size] == 1).all():\n",
    "        label = 1 \n",
    "    else:\n",
    "        print('''\\n CHOOSE OTHER TIMEPONT TO INITIATE POSTPROCESSING\\n ''')\n",
    "        return labels_of_aday\n",
    "    for i in range(window_size,length):\n",
    "        if (labels_of_aday[i-window_size:i] == 0).all(): \n",
    "            label = 0\n",
    "        elif (labels_of_aday[i-window_size:i] == 1).all():\n",
    "            label = 1 \n",
    "        postp_labels[i] = label\n",
    "      \n",
    "    return postp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_starts(vec):\n",
    "    #vec should be day_X_withlight_withtime\n",
    "\n",
    "    length = len(vec)\n",
    "    \n",
    "    #find start indices\n",
    "    #initiate vector with day_starts \n",
    "    day_starts = np.array([0])\n",
    "    #find other day_starts\n",
    "    for i in range(1,length):\n",
    "        if vec[i,4]-vec[i-1,4] >= 1000:\n",
    "            day_starts = np.append(day_starts, np.array([i]))\n",
    "    \n",
    "    #in order to iterate over last day without problems, add another index to day starts, so last day has an end\n",
    "    day_starts = np.append(day_starts, np.array([length]))\n",
    "    return day_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence (vec,seq_length=5):\n",
    "    seq_vec = list()\n",
    "    for i in range(seq_length-1,len(vec)):\n",
    "        seq_vec.append(vec[i-(seq_length-1):i+1])\n",
    "    seq_vec = np.array(seq_vec)\n",
    "    return seq_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return int(math.ceil(x / 10.0)) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(tup1,tup2):\n",
    "    diff = math.sqrt(((tup1[0]-tup2[0])**2)+((tup1[1]-tup2[1])**2)+((tup1[2]-tup2[2])**2))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_to(point,lop,dist=25):\n",
    "    differences = [difference(point,lop[i]) for i in range(len(lop))]\n",
    "    if (np.array(differences) <= dist).any():\n",
    "        return True\n",
    "    else :\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\Sensorlogs.csv\"\n",
    "path_to_data_1 = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\sensorlogs-15-02-2020-07-04-2020.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the raw data and rename columns\n",
    "\n",
    "#load the old data\n",
    "raw_data = pd.read_csv(path_to_data, sep=\";\")\n",
    "raw_data = raw_data.rename(columns = {\"UNIX_TIMESTAMP(logtime)\" : \"logtime\"})\n",
    "\n",
    "# load new data\n",
    "raw_data_1 = pd.read_csv(path_to_data_1, sep=\";\")\n",
    "\n",
    "#concat new and old data \n",
    "raw_data = pd.concat([raw_data,raw_data_1])\n",
    "\n",
    "#rename columns\n",
    "raw_data = raw_data.rename(columns = {\"logtime\" : \"time\", \"ambient_light_voltage\" : \"light\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the time columns as index \n",
    "\n",
    "data = raw_data.set_index(\"time\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the data \n",
    "\n",
    "data_labeled_with_threshhold = data.assign(label=lambda x: (x.light>threshhold_daylight_for_classification)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test and train split \n",
    "\n",
    "X = data_labeled_with_threshhold.iloc[:,:3].to_numpy()\n",
    "y = data_labeled_with_threshhold.iloc[:,4].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=split_train_test, random_state=data_split_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Daylight points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste von Tageslichtpunkten bekommen, da nur diese gelabeled werden können\n",
    "\n",
    "list_of_one_day = list()\n",
    "list_of_daytime = list()\n",
    "\n",
    "# Tagespunkte für einen Tag\n",
    "for i in range(1575534600,1575558600):\n",
    "    list_of_one_day.append(i)\n",
    "\n",
    "# Remove bad days from Training/Test Set\n",
    "list_of_days = [i for i in range(NUMBER_OF_DAYS) if i not in LIST_OF_BAD_DAYS]\n",
    "#Tagespunkte für alle guten Tage \n",
    "for i in list_of_days:\n",
    "    for t in list_of_one_day:\n",
    "        list_of_daytime.append(t+86400*i)\n",
    "\n",
    "day_data = data[data.index.isin(list_of_daytime)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add time as column and convert it to array\n",
    "day_data_withtime = day_data.assign(time=lambda x: x.index)\n",
    "day_X_withlight_withtime = day_data_withtime.iloc[:,:5].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the day_starts \n",
    "day_starts = get_day_starts(day_X_withlight_withtime)\n",
    "\n",
    "# get index from where data is unseen till testing\n",
    "unseen_days_start_at = math.floor(len(day_starts)*(1-PERCENTAGE_OF_UNSEEN_DAYS))\n",
    "unseen_index = day_starts[unseen_days_start_at]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into test and train split and get data for further data analysis\n",
    "\n",
    "#get data into array format \n",
    "day_X = day_data_withtime.iloc[:,:3].to_numpy()\n",
    "\n",
    "#get labels \n",
    "day_y = label_data_light_time_full(day_X_withlight_withtime)\n",
    "\n",
    "\n",
    "#split into test and train set \n",
    "day_X_train, day_X_test, day_y_train, day_y_test = train_test_split( day_X[:unseen_index], day_y[:unseen_index], \n",
    "                                                                    test_size=split_train_test, \n",
    "                                                                    random_state=data_split_seed)\n",
    "\n",
    "day_X_unseen, day_y_unseen = day_X[unseen_index:], day_y[unseen_index:]\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(day_X_train)\n",
    "#day_X_train = scaler.transform(day_X_train)\n",
    "#day_X_test = scaler.transform(day_X_test)\n",
    "#day_X_unseen = scaler.transform(day_X_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also scale day_X_withlight_withtime \n",
    "\n",
    "#for i,x in enumerate(day_X_withlight_withtime):\n",
    "#    day_X_withlight_withtime[i,:3] = scaler.transform([day_X_withlight_withtime[i,:3]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Take a closer look at data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the changes in the light function \n",
    "\n",
    "changes=np.zeros(len(day_y)-1)\n",
    "for i in range(len(day_y)-1):\n",
    "    changes[i]=day_X_withlight_withtime[i,3]-day_X_withlight_withtime[i+1,3]\n",
    "    \n",
    "sum_changes = np.zeros(401)\n",
    "hist, bins = np.histogram(changes,bins=[i for i in range(40,400,1)])\n",
    "\n",
    "for i in range (399,39,-1):\n",
    "    sum_changes[i]=sum_changes[i+1]+hist[i-40-1]\n",
    "#plt.plot([i for i in range(60,120)], sum_changes[60:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liste von Punkten, wo die Tage anfangen generieren\n",
    "\n",
    "list_of_daystarts_oneday = list()\n",
    "list_of_daystarts = list()\n",
    "\n",
    "# Punkte wo erster Tag \"anfängt\"\n",
    "for i in range(1575534600-5,1575534600+6):\n",
    "    list_of_daystarts_oneday.append(i)\n",
    "    \n",
    "#Punkte wo alle Tage anfangen\n",
    "for i in range(72):\n",
    "    for t in list_of_daystarts_oneday:\n",
    "        list_of_daystarts.append(t+86400*i)\n",
    "        \n",
    "daystart_data = data[data.index.isin(list_of_daystarts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hist1 = daystart_data.hist()\n",
    "#hist2 = day_data.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary Classification using day_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(day_X_train, day_y_train) # ? 832 #9204\n",
    "score = round(LR.score(day_X_test,day_y_test), 4)\n",
    "print(\"The score of the Logistic Regression is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_score = round(LR.score(day_X_unseen, day_y_unseen), 4)\n",
    "print(LR_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(gamma='scale', C=1.0 )\n",
    "SVM.fit(day_X_train, day_y_train)\n",
    "score = round(SVM.score(day_X_test,day_y_test), 4)\n",
    "print(\"The score of the SVM is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_score = round(SVM.score(day_X_unseen, day_y_unseen), 4)\n",
    "print(SVM_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0)  9009 9036 # 9861\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=0)\n",
    "RF.fit(day_X_train, day_y_train) \n",
    "score = round(RF.score(day_X_test,day_y_test), 4)\n",
    "print(\"The score of the Random Forest Classfier is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_score = round(RF.score(day_X_unseen, day_y_unseen), 4)\n",
    "print(RF_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 MultiLayerPerceptron (NN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN = MLPClassifier(hidden_layer_sizes=(20, 50, 100, 100, 50, 20), solver='adam', alpha=1e-5,  random_state=1) 0.936\n",
    "# (20,50,100,100,50,20),(30,65,120,120,65,30),(10,30,70,70,30,10) max_iter =200 has  8557   8509     8612\n",
    "# (10,20,30,40,50,60,70,70,60,50,40,30,20,10), (100,90,80,70,60,50,40,30,20,10)  8553 8429\n",
    "# NN_to_test = [(10,30,70,70,30,10),(5,20,55,55,20,5),(20,45,90,90,45,20)] 9279, 9346, 9387\n",
    "# NN_to_test = [(20,30,45,65,90,90,65,45,30,20),(30,60,120,120,60,30)] 9290 9306\n",
    "# (20,45,90,90,45,20) 9505 # 9918\n",
    "# (30,55,110,110,55,30) # 9918\n",
    "# (40,65,130,130,65,40) 9932\n",
    "# (10,10,20,20,20,20,10,10) 9996\n",
    "NN_to_test = [(10,10,20,20,20,20,10,10)]\n",
    "for i in range(len(NN_to_test)):\n",
    "    NN = MLPClassifier(hidden_layer_sizes=NN_to_test[i], solver='adam', alpha=1e-5,  random_state=1, max_iter=250)\n",
    "    NN.fit(day_X_train, day_y_train)\n",
    "    score = round(NN.score(day_X_test,day_y_test), 4)\n",
    "    print(\"The score of the MLP is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_score = round(NN.score(day_X_unseen, day_y_unseen), 4)\n",
    "print(NN_score) \n",
    "\n",
    "#(10,10,20,20,20,20,20,20,10,10) 9996, 9378 \n",
    "#(10,10,20,20,20,20,10,10) 9981 9725\n",
    "# #(10,10,20,20,20,20,10,10) ? 9427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Save and Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\LR.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(LR,dumphere)\n",
    "    \n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\SVM.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(SVM,dumphere)\n",
    "\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\RF.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(RF,dumphere)\n",
    "    \n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\NN.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(NN,dumphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\NN.pkl\"\n",
    "NN = pickle.load(open( output, \"rb\" ))\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\RF.pkl\"\n",
    "RF = pickle.load(open( output, \"rb\" ))\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\SVM.pkl\"\n",
    "SVM = pickle.load(open( output, \"rb\" ))\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\LR.pkl\"\n",
    "LR = pickle.load(open( output, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering using non labeled data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_est = KMeans(n_clusters=2, max_iter=5, n_init=1)\n",
    "kmeans_est.fit(day_X_train)\n",
    "labels = kmeans_est.labels_ \n",
    "score = round(get_score(kmeans_est.predict(day_X_unseen),day_y_unseen),4)\n",
    "\n",
    "# since clustering does not know my classes, it might give them the wrong labels, so i have to get the \"right\" score\n",
    "if score>= 0.5:\n",
    "    score = score \n",
    "else: \n",
    "    score = 1 - score\n",
    "    \n",
    "print(\"The score of K-Means is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw the clustered training points with their labels \n",
    "\n",
    "name = 'Cluster Daten'\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = Axes3D(fig)    \n",
    "ax.scatter(day_X_train[:, 0], day_X_train[:, 1], day_X_train[:, 2], c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification using Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Generation of Sequential Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 5\n",
    "\n",
    "seq_X_day_test = list()\n",
    "seq_y_day_test = list()\n",
    "seq_X_day_train = list()\n",
    "seq_y_day_train = list()\n",
    "seq_light_day_train = list()\n",
    "seq_light_day_test = list()\n",
    "\n",
    "random.seed(234250982)\n",
    "train_and_test = [i for i in range(len(day_starts)-1)]\n",
    "random.shuffle(train_and_test)\n",
    "\n",
    "counter = 0\n",
    "i = SEQUENCE_LENGTH-1\n",
    "while i < len(day_y)/2:\n",
    "    if day_X_withlight_withtime[i,4]-day_X_withlight_withtime[i-(SEQUENCE_LENGTH-1),4] < 200:\n",
    "        if counter in train_and_test[:50]:\n",
    "            seq_X_day_train.append([x  for y in day_X_withlight_withtime[i-(SEQUENCE_LENGTH-1):i+1,:3] for x in y ])\n",
    "            seq_y_day_train.append(day_y[i])\n",
    "            seq_light_day_train.append(day_X_withlight_withtime[i,3])\n",
    "        else: \n",
    "            seq_X_day_test.append([x for y in day_X_withlight_withtime[i-(SEQUENCE_LENGTH-1):i+1,:3] for x in y ])\n",
    "            seq_y_day_test.append(day_y[i])\n",
    "            seq_light_day_test.append(day_X_withlight_withtime[i,3])\n",
    "        i += 1\n",
    "    else:\n",
    "        counter += 1\n",
    "        i += (SEQUENCE_LENGTH-1)\n",
    " \n",
    "\n",
    "\n",
    "seq_X_day_test = np.array(seq_X_day_test)\n",
    "seq_y_day_test = np.array(seq_y_day_test)\n",
    "seq_X_day_train = np.array(seq_X_day_train)\n",
    "seq_y_day_train = np.array(seq_y_day_train)\n",
    "\n",
    "scaler_seq = StandardScaler()\n",
    "scaler_seq = scaler.fit(seq_X_day_train)\n",
    "seq_X_day_train = scaler.transform(seq_X_day_train)\n",
    "seq_X_day_test = scaler.transform(seq_X_day_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_seq = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=200) # 7323\n",
    "LR_seq.fit(seq_X_day_train, seq_y_day_train)\n",
    "score = round(LR_seq.score(seq_X_day_test, seq_y_day_test), 4)\n",
    "print(\"The score of the Logistic Regression sequentual is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_seq = svm.SVC(gamma='scale', C=1.0 , max_iter = 10000) # \n",
    "SVM_seq.fit(seq_X_day_train, seq_y_day_train)\n",
    "score = round(SVM_seq.score(seq_X_day_test, seq_y_day_test), 4)\n",
    "print(\"The score of the SVM sequential is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_seq = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0) #8228\n",
    "RF_seq.fit(seq_X_day_train, seq_y_day_train) \n",
    "score = round(RF_seq.score(seq_X_day_test, seq_y_day_test), 4)\n",
    "print(\"The score of the Random Forest Classfier sequential is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_to_test = [(50,80,100,150,100,70,20),(20,50,100,60,30),(50,150,300,100,20)] 0.866 ...\n",
    "# NN_to_test = [(50,100,150,300,150,100,5 0,20)]\n",
    "# (50,80,100,100,70,50)\n",
    "# (10,20,20,20,20,20,10) 9638\n",
    "NN_to_test = [(10,20,20,20,20,20,20,20,20,10)] #8613\n",
    "for i in range(len(NN_to_test)):\n",
    "    NN_seq = MLPClassifier(hidden_layer_sizes=NN_to_test[i], solver='adam', alpha=1e-5,  random_state=1, max_iter=250)\n",
    "    NN_seq.fit(seq_X_day_train, seq_y_day_train)\n",
    "    score = round(NN_seq.score(seq_X_day_test, seq_y_day_test), 4)\n",
    "    print(\"The score of the NN sequential is: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Save and load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\LR_seq.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(LR_seq,dumphere)\n",
    "    \n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\SVM_seq.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(SVM_seq,dumphere)\n",
    "\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\RF_seq.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(RF_seq,dumphere)\n",
    "    \n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\NN_seq.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(NN_seq,dumphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\LR_seq.pkl\"\n",
    "LR_seq = pickle.load(open( output, \"rb\" ))\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\SVM_seq.pkl\"\n",
    "SVM_seq = pickle.load(open( output, \"rb\" ))\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\RF_seq.pkl\"\n",
    "RF_seq = pickle.load(open( output, \"rb\" ))\n",
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\NN_seq.pkl\"\n",
    "NN_seq = pickle.load(open( output, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Error Analysis of Sequential Data Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diam = 12\n",
    "radius = 6\n",
    "#for i, row in enumerate(day_X): \n",
    "#    if day_y_new[i]!=day_y_new[i-1]:\n",
    "#        print(\".........\")\n",
    "#        for j in range(diam):\n",
    "#            print(day_y_new[i-radius+j],day_X_withlight_withtime[i-radius+j,:3],day_X_withlight_withtime[i-radius+j,3],\n",
    "#                 day_X_withlight_withtime[i-radius+j,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification with RNN using keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, TimeDistributed, SimpleRNN\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Getting Data into Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data into format\n",
    "rnn_X_day_test = list()\n",
    "rnn_y_day_test = list()\n",
    "rnn_X_day_train = list()\n",
    "rnn_y_day_train = list()\n",
    "rnn_light_day_train = list()\n",
    "rnn_light_day_test = list()\n",
    "\n",
    "random.seed(200537987)\n",
    "train_and_test = [i for i in range (0,len(day_starts))]\n",
    "# random.shuffle(train_and_test)\n",
    "\n",
    "counter = 0\n",
    "i = 4 \n",
    "\n",
    "while i < len(day_y):\n",
    "    if day_X_withlight_withtime[i,4]-day_X_withlight_withtime[i-4,4] < 200:\n",
    "        if counter in train_and_test[:unseen_days_start_at]:\n",
    "            rnn_X_day_train.append(day_X_withlight_withtime[i-4:i+1,:3])\n",
    "            rnn_y_day_train.append(day_y[i])\n",
    "            rnn_light_day_train.append(day_X_withlight_withtime[i,3])\n",
    "        else: \n",
    "            rnn_X_day_test.append(day_X_withlight_withtime[i-4:i+1,:3])\n",
    "            rnn_y_day_test.append(day_y[i])     \n",
    "            rnn_light_day_test.append(day_X_withlight_withtime[i,3])\n",
    "        i += 1\n",
    "    else:\n",
    "        counter += 1\n",
    "        i += 4\n",
    "\n",
    "\n",
    "\n",
    "rnn_X_day_train = np.array(rnn_X_day_train)\n",
    "rnn_y_day_train = np.array(rnn_y_day_train)\n",
    "rnn_X_day_test = np.array(rnn_X_day_test)\n",
    "rnn_y_day_test = np.array(rnn_y_day_test)\n",
    "rnn_y_day_train = to_categorical(rnn_y_day_train)\n",
    "rnn_y_day_test = to_categorical(rnn_y_day_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Generating RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(30,input_shape=(5,3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adadelta', metrics=['accuracy'])\n",
    "\n",
    "# 200, 150, 100, 50, 9940 9339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(rnn_X_day_train, rnn_y_day_train,batch_size=25, epochs=5, verbose=1 )\n",
    "\n",
    "_, RNN_score = model.evaluate(rnn_X_day_test, rnn_y_day_test, verbose=0)\n",
    "RNN_score = round(RNN_score,4)\n",
    "print(RNN_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Save and Load the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = r\"C:\\Users\\clhco\\Projekte\\SamsParkplatzMagnet\\RNN.pkl\"\n",
    "with open(output,\"wb\") as dumphere:\n",
    "    pickle.dump(RNN,dumphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Error Analysis of RNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Sequence, prediction, light, truth')\n",
    "#for i,seq in enumerate(rnn_X_day_test):\n",
    "#    if (get_label(model.predict(np.array([seq]))) != rnn_y_day_test[i]).all():\n",
    "#        print(seq,label_to_num(get_label(model.predict(np.array([seq])))),\n",
    "#              rnn_light_day_test[i-4:i+1],label_to_num(rnn_y_day_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=np.array([0,0])\n",
    "for i,seq in enumerate(rnn_X_day_test):\n",
    "    if label_to_num(get_label(model.predict(np.array([seq]))))==0: \n",
    "        count[0] += 1 \n",
    "    else:\n",
    "        count[1] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Error Analysis using graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Plotting time, label and prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SHOW_MAGNETS = False\n",
    "HEIGHT_OF_GRAPHS = 3\n",
    "CLASSIFIERS_TO_EVAL = ['SVM']\n",
    "vec = day_X_withlight_withtime\n",
    "\n",
    "for i in range(100,110):\n",
    "    \n",
    "    print(i)\n",
    "    labels = [(2-label)*500 for label in day_y[day_starts[i]:day_starts[i+1]]]\n",
    "    \n",
    "    for classifier in CLASSIFIERS_TO_EVAL:\n",
    "        \n",
    "        if classifier == 'model':\n",
    "            \n",
    "            predicted_labels = [np.argmax(tup)\n",
    "                                for tup in eval(classifier+'.predict(make_sequence(vec[day_starts[i]:day_starts[i+1],:3]))')]\n",
    "            score = round(get_score(predicted_labels,day_y[day_starts[i]+4:day_starts[i+1]]),4)\n",
    "            predicted_labels = postprocessing_of_labels(predicted_labels)\n",
    "            score_p = round(get_score(predicted_labels,day_y[day_starts[i]+4:day_starts[i+1]]),4)\n",
    "            predicted_labels = [(2-label)*500 for label in predicted_labels]\n",
    "\n",
    "            print('RNN')\n",
    "            print(score)\n",
    "            print(score_p)\n",
    "    \n",
    "            plt.figure(figsize=(30, HEIGHT_OF_GRAPHS), dpi=80)\n",
    "            plt.plot(vec[day_starts[i]+5:day_starts[i+1],3])\n",
    "            plt.plot(labels,'.')\n",
    "            plt.plot(predicted_labels,'--')\n",
    "            plt.show(block=False)\n",
    "            \n",
    "        elif classifier.endswith('_seq'):\n",
    "            \n",
    "            predicted_labels = eval(classifier+'''.predict(\n",
    "            scaler_seq.transform([ seq.flatten() for seq in make_sequence(vec[day_starts[i]:day_starts[i+1],:3])]))''')\n",
    "            score = round(get_score(predicted_labels,day_y[day_starts[i]+4:day_starts[i+1]]),4)\n",
    "            predicted_labels = postprocessing_of_labels(predicted_labels)\n",
    "            score_p = round(get_score(predicted_labels,day_y[day_starts[i]+4:day_starts[i+1]]),4)\n",
    "            predicted_labels = [(2-label)*500 for label in predicted_labels]\n",
    "\n",
    "            print(classifier)\n",
    "            print(score)\n",
    "            print(score_p)\n",
    "    \n",
    "            plt.figure(figsize=(30, HEIGHT_OF_GRAPHS), dpi=80)\n",
    "            plt.plot(vec[day_starts[i]+5:day_starts[i+1],3])\n",
    "            plt.plot(labels,'.')\n",
    "            plt.plot(predicted_labels,'--')\n",
    "            plt.show(block=False)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "\n",
    "            vec[day_starts[i]:day_starts[i+1],:3]\n",
    "            predicted_labels = eval(classifier+'.predict(vec[day_starts[i]:day_starts[i+1],:3])')\n",
    "            score = round(get_score(predicted_labels,day_y[day_starts[i]:day_starts[i+1]]),4)\n",
    "            predicted_labels = postprocessing_of_labels(predicted_labels)\n",
    "            score_p = round(get_score(predicted_labels,day_y[day_starts[i]:day_starts[i+1]]),4)\n",
    "            predicted_labels = [(2-label)*500 for label in predicted_labels]\n",
    "\n",
    "            print(classifier)\n",
    "            print(score)\n",
    "            print(score_p)\n",
    "    \n",
    "            plt.figure(figsize=(30, HEIGHT_OF_GRAPHS), dpi=80)\n",
    "            plt.plot(vec[day_starts[i]:day_starts[i+1],3])\n",
    "            plt.plot(labels,'.')\n",
    "            plt.plot(predicted_labels,'--')\n",
    "            plt.show(block=False)\n",
    "        \n",
    "        \n",
    "            \n",
    " \n",
    "    \n",
    "    if SHOW_MAGNETS: \n",
    "\n",
    "        print('x')\n",
    "        plt.figure(figsize=(30, HEIGHT_OF_GRAPHS), dpi=80)\n",
    "        plt.plot(vec[day_starts[i]:day_starts[i+1],0])\n",
    "        plt.show(block=False)\n",
    "        print('y')\n",
    "        plt.figure(figsize=(30, HEIGHT_OF_GRAPHS), dpi=80)\n",
    "        plt.plot(vec[day_starts[i]:day_starts[i+1],1])\n",
    "        plt.show(block=False)\n",
    "        print('z')\n",
    "        plt.figure(figsize=(30, HEIGHT_OF_GRAPHS), dpi=80)\n",
    "        plt.plot(vec[day_starts[i]:day_starts[i+1],2])\n",
    "        plt.show(block=False)\n",
    "\n",
    "    \n",
    "\n",
    "# Fehler : \n",
    "# wegen früher Dunkelheit spike nach oben nicht gemerkt: 4, 6,\n",
    "# noise ruckler(kurzes hin und her) : 1, 13, 68 \n",
    "# daher werden 1,4,6,13, 64 aus dem Datensatz entfernt aufgrund von artfakten der magnetspule,\n",
    "#        die durch licht nicht erkannt werden\n",
    "\n",
    "# print(day_starts[100]) = 524044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Plot the distribution of the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the intervalls the values are in \n",
    "\n",
    "vec = day_X_withlight_withtime\n",
    "\n",
    "x_max = vec[:,0].max()\n",
    "x_min = vec[:,0].min()\n",
    "y_max = vec[:,1].max()\n",
    "y_min = vec[:,1].min()\n",
    "z_max = vec[:,2].max()\n",
    "z_min = vec[:,2].min()\n",
    "\n",
    "# print(x_max,x_min,y_max,y_min,z_max,z_min) 1073 64 -318 -1375 1162 -1343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the points in the dataset with their labels \n",
    "vec = day_X_withlight_withtime\n",
    "\n",
    "points_to_paint_0 = list()\n",
    "points_to_paint_1 = list()\n",
    "\n",
    "for i,ts in enumerate(vec):\n",
    "    if day_y[i] == 0:\n",
    "        xyz=[roundup(x) for x in ts[:3]]\n",
    "        if xyz not in points_to_paint_0:\n",
    "            points_to_paint_0.append(xyz)\n",
    "    else: \n",
    "        xyz=[roundup(x) for x in ts[:3]]\n",
    "        if xyz not in points_to_paint_1:\n",
    "            points_to_paint_1.append(xyz)  \n",
    "    \n",
    "list_of_times = list()\n",
    "for i,tx in enumerate(vec):\n",
    "    xyz = [roundup(x) for x in ts[:3]]\n",
    "    if close_to(xyz,points_to_paint_0,dist=0):\n",
    "        list_of_times.append(ts[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_of_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_right_times = [datetime.fromtimestamp(list_of_times[i]) for i in range(len(list_of_times))]\n",
    "print(list_of_right_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_right_times = sorted(list_of_right_times)\n",
    "print(list_of_right_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(points_to_paint_0),len(points_to_paint_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot the points in the dataset with their label\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for point in points_to_paint_0:\n",
    "    xs = point[0]\n",
    "    ys = point[1]\n",
    "    zs = point[2]\n",
    "    ax.scatter(xs, ys, zs, s=20, color='yellow', edgecolor='k')\n",
    "\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "ax.set_xlim(x_min,x_max)\n",
    "ax.set_ylim(y_min,y_max)\n",
    "ax.set_zlim(z_min,z_max)\n",
    "\n",
    "plt.show(block=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "for point in points_to_paint_1:\n",
    "    if close_to(point,points_to_paint_0, dist=0):\n",
    "        xs = point[0]\n",
    "        ys = point[1]\n",
    "        zs = point[2]\n",
    "        ax.scatter(xs, ys, zs, s=20, color='blue', edgecolor='k')\n",
    "        print(point)\n",
    "        \n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "ax.set_xlim(x_min,x_max)\n",
    "ax.set_ylim(y_min,y_max)\n",
    "ax.set_zlim(z_min,z_max)\n",
    "\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the points and label them \n",
    "\n",
    "PLOT_TRUTH = True\n",
    "\n",
    "for classifier in ['LR','SVM','RF','NN']:\n",
    "    \n",
    "    print(classifier)\n",
    "    \n",
    "    points = list()\n",
    "\n",
    "    for x in range(x_min,x_max):\n",
    "        for y in range(y_min,y_max):\n",
    "            for z in range(z_min,z_max):\n",
    "                points.append([x,y,z])\n",
    "\n",
    "    predict_0 = list() \n",
    "    predict_1 = list()\n",
    "    for point in points:\n",
    "        lable_1 = eval(classifier+'.predict([point])[0]')\n",
    "        if lable_1 == 0:\n",
    "            predict_0.append(point)\n",
    "        else:\n",
    "            predict_1.append(point)\n",
    "    print(predict_0[0],predict_0[-1])\n",
    "    #plot the points\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    colors=['blue','yellow']\n",
    "    for label,labeled_vec in enumerate([predict_0]):\n",
    "        for vector in labeled_vec:\n",
    "            xs = vector[0]\n",
    "            ys = vector[1]\n",
    "            zs = vector[2]\n",
    "            color = colors[label]\n",
    "            ax.scatter(xs, ys, zs, s=20,c=color,edgecolor = 'k')\n",
    "    \n",
    "    if PLOT_TRUTH:\n",
    "        for point in points_to_paint:\n",
    "            xs = point[0]\n",
    "            ys = point[1]\n",
    "            zs = point[2]\n",
    "            ax.scatter(xs, ys, zs, s=20, color='yellow',edgecolor = 'k')\n",
    "        \n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,y_max)\n",
    "    ax.set_zlim(z_min,z_max)\n",
    "\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The score of the Logistic Regression on unseen data is : {}\".format(RF_score))\n",
    "print(\"The score of the Support Vector Machine on unseen data is: {}\".format(SVM_score))\n",
    "print(\"The score of the Random Forst Classifier on unseen data is: {}\".format(RF_score))\n",
    "print(\"The score of the Neural Network on unseen data is: {}\".format(NN_score))\n",
    "print(\"The score of the RNN on unseen data is: {}\".format(RNN_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0): \n",
    "    plt.figure(figsize=(30, 10), dpi=80)\n",
    "    plt.plot(smooth(day_X_withlight_withtime[day_starts[i]:day_starts[i+1],3],window_len = 10000))\n",
    "    plt.plot(day_X_withlight_withtime[day_starts[i]:day_starts[i+1],3])\n",
    "    plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}